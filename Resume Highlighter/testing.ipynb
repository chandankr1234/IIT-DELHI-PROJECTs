{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute all cells in the notebook up to the last one. For the final cell, you will need to supply values for the following variables to test the model:\n",
    "- `output_pdf_path`: Specify the file path where the output PDF will be saved.\n",
    "- `pdf_path`: Enter the path to the PDF file of the resume you wish to test.\n",
    "- `resume_category`: Assign the desired job category for the resume classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T14:39:15.979210Z",
     "iopub.status.busy": "2024-03-27T14:39:15.978524Z",
     "iopub.status.idle": "2024-03-27T14:39:34.640471Z",
     "shell.execute_reply": "2024-03-27T14:39:34.638461Z",
     "shell.execute_reply.started": "2024-03-27T14:39:15.979167Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T14:42:59.593537Z",
     "iopub.status.busy": "2024-03-27T14:42:59.593078Z",
     "iopub.status.idle": "2024-03-27T14:42:59.680575Z",
     "shell.execute_reply": "2024-03-27T14:42:59.679598Z",
     "shell.execute_reply.started": "2024-03-27T14:42:59.593503Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "import re\n",
    "import fitz  # Import the PyMuPDF library\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T15:18:43.427214Z",
     "iopub.status.busy": "2024-03-27T15:18:43.426257Z",
     "iopub.status.idle": "2024-03-27T15:18:44.685826Z",
     "shell.execute_reply": "2024-03-27T15:18:44.684640Z",
     "shell.execute_reply.started": "2024-03-27T15:18:43.427178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'model' is your fine-tuned BertModel instance\n",
    "# Load tokenizer and model (replace 'bert-base-uncased' with your model's name if you've saved it under a different name)\n",
    "df1=pd.read_csv('worldcities.csv')\n",
    "df1=df1[(df1['country']=='India') | (df1['country'] == 'United States')]\n",
    "cities=df1['city'].tolist()\n",
    "for i in range(0,len(cities)):\n",
    "  cities[i]=cities[i].lower()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('BERT_FINETUNED')  # Or wherever you've saved your fine-tuned model\n",
    "\n",
    "def get_average_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Masked embeddings: zero out padding tokens\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "\n",
    "    # Sum over the sequence length dimension and divide by the number of non-padding tokens\n",
    "    summed_embeddings = masked_embeddings.sum(1)\n",
    "    num_non_padding_tokens = mask.sum(1)\n",
    "    average_embedding = summed_embeddings / num_non_padding_tokens\n",
    "\n",
    "    return average_embedding\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# List of months in long format\n",
    "# List of months in long format with all alphabets in lowercase\n",
    "months_long= [\n",
    "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
    "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
    "]\n",
    "\n",
    "# List of months in short format with all alphabets in lowercase\n",
    "months_short = [\n",
    "    \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\",\n",
    "    \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"\n",
    "]\n",
    "# Extend the stopwords list with your custom deletion words\n",
    "del_words = ['name', 'city', 'state', 'country', 'fullname', 'company', 'resume','intro', 'curriculum', 'vitae', 'address', 'phone',\n",
    "             'email', 'linkedin', 'profile', 'summary', 'objective', 'experience', 'education', 'skill', 'skills','bachelor',\n",
    "             'reference', 'references', 'contact', 'detail', 'details', 'mail', 'gmail', 'yahoo', 'hotmail', 'mailing',\n",
    "             'twitter', 'facebook', 'instagram','intro','using', 'website', 'web', 'url', 'www', 'year', 'month','months','requirement','first', 'last', 'xxxx', 'rstlast', 'rstlast', 'github', 'rstlast', 'university', 'expected', 'bachelor', 'science','project', 'description', 'responsibility', 'role','time','nagpur', 'secondary','exprience']\n",
    "\n",
    "stop_words = stopwords.words('english') + del_words +cities+months_long+months_short\n",
    "# ct=0\n",
    "def preprocess(text):\n",
    "    text = re.sub('http\\S+\\s*', ' ', text)  # remove URLs\n",
    "    text = re.sub('RT|cc', ' ', text)  # remove RT and cc\n",
    "    text = re.sub('#\\S+', '', text)  # remove hashtags\n",
    "    text = re.sub('@\\S+', '  ', text)  # remove mentions\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuations\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ', text)  # remove non-ASCII characters\n",
    "    text = re.sub('\\s+', ' ', text)  # remove extra whitespace\n",
    "    text = re.sub('\\d+', '', text)  # remove numbers\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "              result.append(token)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def cosine_similarity_torch(vec_a, vec_b):\n",
    "    # Calculate the dot product of the two vectors\n",
    "    dot_product = torch.dot(vec_a, vec_b)\n",
    "\n",
    "    # Calculate the magnitude (norm) of each vector\n",
    "    norm_a = torch.norm(vec_a)\n",
    "    norm_b = torch.norm(vec_b)\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T15:26:02.479802Z",
     "iopub.status.busy": "2024-03-27T15:26:02.479390Z",
     "iopub.status.idle": "2024-03-27T15:26:02.497008Z",
     "shell.execute_reply": "2024-03-27T15:26:02.495721Z",
     "shell.execute_reply.started": "2024-03-27T15:26:02.479774Z"
    }
   },
   "outputs": [],
   "source": [
    "def testing_model(output_pdf_path,pdf_path,resume_category):\n",
    "    \n",
    "    resume_category=preprocess(resume_category)\n",
    "    resume_category=get_average_embedding(resume_category, tokenizer, model)\n",
    "    def read_pdf_line_by_line_cleaned(pdf_path):\n",
    "        all_chars_to_remove = string.punctuation.replace('.', '')\n",
    "        remove_chars_trans = str.maketrans('', '', all_chars_to_remove)\n",
    "\n",
    "        # Open the PDF file\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        # Iterate through each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "\n",
    "            # Extract text from the page\n",
    "            text = page.get_text()\n",
    "\n",
    "            # Split the text into lines\n",
    "            lines = text.split('\\n')\n",
    "            pp=0\n",
    "            ans={}\n",
    "            # Iterate through each line, remove specified characters, then filter out non-ASCII characters\n",
    "            for line in lines:\n",
    "    #             print(pp)\n",
    "                pp+=1\n",
    "                clean_line = line.translate(remove_chars_trans)\n",
    "                # Replace .js with empty string in each word\n",
    "                clean_line = ' '.join(word.replace('.js', '') for word in clean_line.split())\n",
    "                # Filter out non-ASCII characters\n",
    "                clean_line = ''.join(char for char in clean_line if ord(char) < 128)\n",
    "                chunks = [sentence.strip() for sentence in re.split('[.]', clean_line) if sentence]\n",
    "                for chunk in chunks:\n",
    "                    chunk=preprocess(chunk)\n",
    "                    if len(chunk)>0:\n",
    "                        similiarity=cosine_similarity_torch(get_average_embedding(chunk,tokenizer, model)[0],resume_category[0])\n",
    "                        ans[chunk]=similiarity\n",
    "        doc.close()\n",
    "        return ans\n",
    "\n",
    "    # Read and print each line from the PDF\n",
    "    c=read_pdf_line_by_line_cleaned(pdf_path)\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "#     print(c)\n",
    "    # Get the number of pages\n",
    "    num_pages = len(doc)\n",
    "\n",
    "    print(f\"The number of pages in the PDF is: {num_pages}\")\n",
    "\n",
    "    # Close the document if it's no longer needed\n",
    "    doc.close()\n",
    "\n",
    "    # Sort the dictionary by value and pick the top 10 key-value pairs\n",
    "    sorted_c = dict(sorted(c.items(), key=lambda item: item[1], reverse=True)[:(10*num_pages)])\n",
    "    def highlight_words_in_pdf(input_pdf_path, output_pdf_path, words_dict):\n",
    "        doc = fitz.open(input_pdf_path)\n",
    "\n",
    "        for page in doc:  # Iterate over all pages\n",
    "            page.clean_contents()\n",
    "            for sentence, key in words_dict.items():\n",
    "                words = sentence.split()  # Split the sentence into words\n",
    "                for word in words:\n",
    "                    text_instances = page.search_for(word)  # Search for the word in the page\n",
    "\n",
    "                    # Highlight each instance found\n",
    "                    for inst in text_instances:\n",
    "                        highlight = page.add_highlight_annot(inst)\n",
    "                        highlight.set_colors(stroke=(1, key, 0))  # Set color to yellow (R, G, B)\n",
    "                        highlight.update()\n",
    "\n",
    "        doc.save(output_pdf_path)\n",
    "        doc.close()\n",
    "    highlight_words_in_pdf(pdf_path,output_pdf_path,sorted_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T15:26:04.942315Z",
     "iopub.status.busy": "2024-03-27T15:26:04.941934Z",
     "iopub.status.idle": "2024-03-27T15:26:04.947765Z",
     "shell.execute_reply": "2024-03-27T15:26:04.946619Z",
     "shell.execute_reply.started": "2024-03-27T15:26:04.942286Z"
    }
   },
   "outputs": [],
   "source": [
    "output_pdf_path='highlighted2.pdf'\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = 'Labelled/SWE_Resume_Highlighted.pdf'\n",
    "resume_category='Senior Technical Artist and VFX Specialist with extensive experience in game development across multiple platforms and engines.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-27T15:26:06.675299Z",
     "iopub.status.busy": "2024-03-27T15:26:06.674891Z",
     "iopub.status.idle": "2024-03-27T15:26:13.073433Z",
     "shell.execute_reply": "2024-03-27T15:26:13.072185Z",
     "shell.execute_reply.started": "2024-03-27T15:26:06.675268Z"
    }
   },
   "outputs": [],
   "source": [
    "testing_model(output_pdf_path,pdf_path,resume_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer and model with the Mistral model ID\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "# Define the task and instruction for the model\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"[INST] Generate a list of keywords relevant for a Data Analysis role, separated by commas. [/INST]\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the chat template to format the input correctly\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(inputs, max_new_tokens=100)\n",
    "\n",
    "# Decode and print the generated response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4676871,
     "sourceId": 7952452,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4680742,
     "sourceId": 7957582,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4680849,
     "sourceId": 7957730,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4677050,
     "sourceId": 7952707,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
